# -*- coding: utf-8 -*-
#todo: either make it fast through C++ or optimize or user easy.py  + f score
#interesting: the learning curve is much faster
import sys
sys.path.insert(0, './../lib')
import svm
from svmutil import *
import random
from utils import *
from operator import itemgetter
import concurrent.futures
import os
import math
from svm_helper import *
import time
start_time = time.time()

#todo consider implementing stratified cross validation - but i think it will avarage out
def partition(y, x, k): 
    assert len(y) == len(x)
    assert k > 1
    yx = list(zip(y, x))
    random.shuffle(yx)
    size = int(len(y)/k)
    assert size > 1
    train_yx = yx[:len(yx)-size]
    validation_yx = yx[len(yx)-size:]
    assert len(train_yx) + len(validation_yx) == len(yx)
    return train_yx, validation_yx
    
    
def evaluate_params(train_yx, validation_yx, c_exp, g_exp, max_idx):
    y_train, x_train = zip(*train_yx)
    model = train(y_train, x_train, c_exp, g_exp, max_idx)
    y_valid, x_valid = zip(*validation_yx)
    valid_prediction = predict(model, x_valid)
    return f_score_from_data(valid_prediction, y_valid)
        
def evalForC(y, x, c_exp, g_exp_begin, g_exp_end, g_exp_step, k, max_idx):
    print("eval for " + str(c_exp))
    for g_exp in range(g_exp_begin, g_exp_end, g_exp_step):
        f_sum_valid = 0
        f_sum_train = 0
        for i in range(k):
            train_yx, validation_yx = partition(y, x, k)
            f_valid = evaluate_params(train_yx, validation_yx, c_exp, g_exp, max_idx)
            f_sum_valid += f_valid
            
        yield (c_exp, g_exp, f_sum_valid/k) ##todo collect precision and  recall

def evalForCBind(c_exp, y, x, k):
    x_nodes, idxs = zip(*map(lambda xi: gen_svm_nodearray(xi, isKernel=False), x))
    max_idx = max(idxs)
    g_exp_begin, g_exp_end, g_exp_step =  3, -15, -2
    return list(evalForC(y, x_nodes, c_exp, g_exp_begin, g_exp_end, g_exp_step, k, max_idx))
    
def main():

    c_exp_begin, c_exp_end, c_exp_step = -5,  15,  2

    
    training_file = "../../features_output/train_samples.txt"
    y, x = svm_read_problem(training_file)
    print("Number of samples: " + str(len(y)))
    ranges = find_min_max(x)
    #save_range_file("./ranges.txt", ranges)
    x = scale_input(x, ranges)
   
    
    k = 5 #k-fold cross validation
   
    all_scores = []    
    with concurrent.futures.ProcessPoolExecutor(max_workers=2) as executor:
        c_exps = range(c_exp_begin, c_exp_end, c_exp_step)
        future_scores = {executor.submit(evalForCBind, c_exp, y, x, k): c_exp for c_exp in c_exps}

        print("all submitted")
        for future in concurrent.futures.as_completed(future_scores):
            all_scores += future.result()


    valids = list(filter(lambda score: math.isnan(score[2]) == False, all_scores))
    print(valids)
    best = max(valids, key=itemgetter(2)) 
    print("Best c: " + str(2**best[0]))
    print("Best g: " + str(2**best[1]))
    print("Best f validation score: " + str(best[3]))
    
    with open('cross_vaildation_result.txt', 'w') as f:
        f.write(str(best[0]) + "\n")
        f.write(str(best[1]) + "\n")
        f.write(str(best[2]) + "\n")
        
main()

print("--- %s seconds ---" % (time.time() - start_time))